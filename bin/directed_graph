#!/usr/bin/env python

import os.path, sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import argparse
import time
import json
import cv2
import numpy as np
import scipy.spatial.distance as dist
import networkx as nx

from opensfm import dataset

def load_reconstructions(data):
    with open(data.reconstruction_file()) as fin:
        return json.load(fin)

def optical_center(shot):
    R = cv2.Rodrigues(np.array(shot['rotation'], dtype=float))[0]
    t = shot['translation']
    return -R.T.dot(t)

def viewing_direction(shot):
    R = cv2.Rodrigues(np.array(shot['rotation'], dtype=float))[0]
    t = np.array([0, 0, 1])
    return R.T.dot(t)

def vector2_angle(x1, y1, x2, y2):
    angle = np.arctan2(y1, x1) - np.arctan2(y2, x2)
    if angle > np.pi:
        return angle - 2 * np.pi
    elif angle < -np.pi:
        return angle + 2 * np.pi
    else:
        return angle

def angle_diff(a1, a2):
    diff = np.abs(a1 - a2)

    # Ensure that the angle difference is less than Pi
    diff = np.min(np.vstack((diff, 2*np.pi - diff)), 0)
    return diff

def get_direction_turns_and_motion_angles(min_distance, max_distance, distances, position, direction, positions, directions):

    # Order positions within interval according to shortest distance
    position_indices = np.where((min_distance < distances) & (distances < max_distance))[0]
    ordered_indices = position_indices[np.argsort(distances[position_indices])]

    motions = positions[ordered_indices] - position
    motion_angles = np.array(
        [vector2_angle(direction[0], direction[1], motion[0], motion[1]) for motion in motions],
        float)
    direction_turns = np.array(
        [vector2_angle(direction[0], direction[1], other_direction[0], other_direction[1])
         for other_direction in directions[ordered_indices]],
        float)

    return ordered_indices, direction_turns, motion_angles

def create_optical_center_graph(reconstructions):

    optical_centers = []
    viewing_directions = []
    shot_ids = []

    for reconstruction in reconstructions:
        for shot_id in reconstruction['shots'].keys():
            shot = reconstruction['shots'][shot_id]
            optical_centers.append(optical_center(shot))
            viewing_directions.append(viewing_direction(shot))
            shot_ids.append(shot_id)

    optical_centers = np.vstack(optical_centers)
    viewing_directions = np.vstack(viewing_directions)

    D = dist.squareform(dist.pdist(optical_centers))

    min_distance = 0.01
    step_max_distance = 20
    step_viewing_threshold = 0.25
    step_drift_threshold = 0.5

    turn_max_distance = 15
    turn_viewing_threshold = 0.7

    direction = 'direction'
    viewing = 'viewing'
    drift = 'drift'

    steps = {
        'step_forward': {direction: 0, viewing: step_viewing_threshold, drift: step_drift_threshold},
        'step_backward': {direction: np.pi, viewing: 2 * step_viewing_threshold, drift: step_drift_threshold},
        'step_left': {direction: -np.pi / 2, viewing: 2 * step_viewing_threshold, drift: step_drift_threshold},
        'step_right': {direction: np.pi / 2, viewing: 2 * step_viewing_threshold, drift: step_drift_threshold}
    }

    turns = {
        'turn_left': {direction: -np.pi / 2, viewing: turn_viewing_threshold},
        'turn_right': {direction: np.pi / 2, viewing: turn_viewing_threshold},
        'turn_u': {direction: np.pi, viewing: turn_viewing_threshold}
    }

    graph = nx.DiGraph()

    for i in range(0, D.shape[0]):

        distances = D[i, :]
        oc = optical_centers[i]
        vd = viewing_directions[i]

        ordered_indices, viewing_turns, motion_angles = get_direction_turns_and_motion_angles(
            min_distance, step_max_distance, distances, oc, vd, optical_centers, viewing_directions
        )

        for key in steps:
            step = steps[key]
            motion_drift = angle_diff(motion_angles, step[direction])
            motion_drift = np.max(np.abs(np.vstack((motion_drift - viewing_turns, viewing_turns))), 0)

            step_indices = np.where((motion_drift < step[drift]) & (np.abs(viewing_turns) < step[viewing]))[0]

            if len(step_indices) > 0:
                step_index = ordered_indices[step_indices[0]]
                graph.add_edge(shot_ids[i], shot_ids[step_index], weight=distances[step_index], direction=key)

        ordered_indices, viewing_turns, motion_angles = get_direction_turns_and_motion_angles(
            min_distance, turn_max_distance, distances, oc, vd, optical_centers, viewing_directions
        )

        for key in turns:
            turn = turns[key]
            viewing_diff = angle_diff(viewing_turns, turn[direction])
            turn_indices = np.where((viewing_diff < turn[viewing]))[0]

            if len(turn_indices) > 0:
                turn_index = ordered_indices[turn_indices[0]]
                graph.add_edge(shot_ids[i], shot_ids[turn_index], weight=distances[turn_index], direction=key)

    return graph

if __name__ == "__main__":
    start = time.time()
    parser = argparse.ArgumentParser(description='Compute directed graph from reconstruction')
    parser.add_argument('dataset',
                        help='path to the dataset to be processed')
    args = parser.parse_args()

    data = dataset.DataSet(args.dataset)
    recs = load_reconstructions(data)
    directed_graphs = []

    rec_graph = create_optical_center_graph(recs)
    directed_graph = {
        'edges': {},
        'nodes': list(rec_graph.node)
    }

    for edge in rec_graph.edge:
        directed_graph['edges'][edge] = rec_graph.edge[edge]

    directed_graphs.append(directed_graph)

    with open(data.directed_graph(), 'w') as fout:
        fout.write(json.dumps(directed_graphs))

    end = time.time()
    with open(data.profile_log(), 'a') as fout:
        fout.write('Directed graphs: {0}\n'.format(end - start))


